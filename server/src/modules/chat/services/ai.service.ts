import { GoogleGenerativeAI, Content, FunctionCallingMode } from '@google/generative-ai'
import { CHAT_FUNCTIONS, executeFunction } from './chat-functions.js'
import { chatCacheService } from './chat-cache.service.js'

const GEMINI_API_KEY = process.env.GEMINI_API_KEY

const SYSTEM_PROMPT = `B·∫°n l√† tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa h·ªá th·ªëng qu·∫£n l√Ω b·∫øn xe kh√°ch.

**Vai tr√≤:**
- H·ªó tr·ª£ ng∆∞·ªùi d√πng tra c·ª©u th√¥ng tin v·ªÅ xe, t√†i x·∫ø, tuy·∫øn ƒë∆∞·ªùng, ph√π hi·ªáu, d·ªãch v·ª•
- Gi·∫£i th√≠ch quy tr√¨nh nghi·ªáp v·ª• ƒëi·ªÅu ƒë·ªô xe
- Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ h·ªá th·ªëng qu·∫£n l√Ω b·∫øn xe

**Quy t·∫Øc quan tr·ªçng:**
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu
- LU√îN s·ª≠ d·ª•ng function calling ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ th√¥ng tin c·ª• th·ªÉ
- Khi c√≥ k·∫øt qu·∫£ t·ª´ function call, h√£y format th√¥ng tin m·ªôt c√°ch r√µ r√†ng v·ªõi markdown
- N·∫øu kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu, h√£y g·ª£i √Ω c√°ch t√¨m ki·∫øm kh√°c
- KH√îNG BAO GI·ªú n√≥i "h·ªá th·ªëng b·∫≠n" hay t·ª´ ch·ªëi tr·∫£ l·ªùi

**Format output:**
- S·ª≠ d·ª•ng **bold** cho t√™n xe, t√†i x·∫ø, ƒë∆°n v·ªã
- S·ª≠ d·ª•ng bullet points (-) cho danh s√°ch
- S·ª≠ d·ª•ng emoji ph√π h·ª£p: üöå xe, üë§ t√†i x·∫ø, üè¢ ƒë∆°n v·ªã, üìç tuy·∫øn, üìã d·ªãch v·ª•
- Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, d∆∞·ªõi 300 t·ª´

**H·ªá th·ªëng qu·∫£n l√Ω:**
- Qu·∫£n l√Ω xe kh√°ch, xe bu√Ωt tuy·∫øn c·ªë ƒë·ªãnh
- ƒêi·ªÅu ƒë·ªô xe v√†o/ra b·∫øn
- Qu·∫£n l√Ω t√†i x·∫ø, gi·∫•y ph√©p l√°i xe
- Qu·∫£n l√Ω ƒë∆°n v·ªã v·∫≠n t·∫£i
- C·∫•p ph√π hi·ªáu xe
- Qu·∫£n l√Ω tuy·∫øn ƒë∆∞·ªùng v√† l·ªãch tr√¨nh
- Qu·∫£n l√Ω d·ªãch v·ª•, h√≥a ƒë∆°n, ph√≠

**V√≠ d·ª• c√¢u h·ªèi:**
- "xe 98H07480" ‚Üí g·ªçi search_vehicle
- "t√†i x·∫ø Nguy·ªÖn" ‚Üí g·ªçi search_driver
- "ƒë∆°n v·ªã Ph∆∞∆°ng Trang" ‚Üí g·ªçi search_operator
- "tuy·∫øn S√†i G√≤n ƒê√† L·∫°t" ‚Üí g·ªçi search_route
- "th·ªëng k√™ h√¥m nay" ‚Üí g·ªçi get_dispatch_stats
- "h·ªá th·ªëng c√≥ bao nhi√™u xe" ‚Üí g·ªçi get_system_stats
- "l·ªãch tr√¨nh h√¥m nay" ‚Üí g·ªçi search_schedule
- "d·ªãch v·ª•" ‚Üí g·ªçi search_service`

class AIService {
  private genAI: GoogleGenerativeAI | null = null
  private conversationHistory: Map<string, Content[]> = new Map()

  private getGenAI(): GoogleGenerativeAI {
    if (!this.genAI) {
      if (!GEMINI_API_KEY) {
        throw new Error('GEMINI_API_KEY is not configured')
      }
      this.genAI = new GoogleGenerativeAI(GEMINI_API_KEY)
    }
    return this.genAI
  }

  async generateResponse(
    message: string,
    sessionId: string
  ): Promise<string> {
    try {
      const genAI = this.getGenAI()

      // Get model with function calling enabled
      const model = genAI.getGenerativeModel({
        model: 'gemini-2.5-flash',
        tools: [{ functionDeclarations: CHAT_FUNCTIONS as any }],
        toolConfig: {
          functionCallingConfig: {
            mode: FunctionCallingMode.AUTO
          }
        },
        systemInstruction: SYSTEM_PROMPT
      })

      // Get conversation history
      let history = this.conversationHistory.get(sessionId) || []

      // Start chat with history
      const chat = model.startChat({
        history: history,
        generationConfig: {
          maxOutputTokens: 1500,
          temperature: 0.7,
        },
      })

      // Send message
      let result = await chat.sendMessage(message)
      let response = result.response

      // Check for function calls
      let functionCalls = response.functionCalls()
      let iterations = 0
      const maxIterations = 3

      while (functionCalls && functionCalls.length > 0 && iterations < maxIterations) {
        iterations++
        console.log(`[AI] Function call ${iterations}: ${functionCalls.map(fc => fc.name).join(', ')}`)

        // Execute all function calls
        const functionResponses = await Promise.all(
          functionCalls.map(async (fc) => {
            const result = await executeFunction(fc.name, fc.args as Record<string, any>)
            return {
              functionResponse: {
                name: fc.name,
                response: result
              }
            }
          })
        )

        // Send function results back to model
        result = await chat.sendMessage(functionResponses)
        response = result.response
        functionCalls = response.functionCalls()
      }

      const responseText = response.text()

      // Update history (keep last 20 messages = 10 exchanges)
      history = [...history, { role: 'user', parts: [{ text: message }] }, { role: 'model', parts: [{ text: responseText }] }]
      if (history.length > 20) {
        history = history.slice(-20)
      }
      this.conversationHistory.set(sessionId, history)

      return responseText
    } catch (error: any) {
      console.error('AI Service error:', error?.message || error)
      console.error('AI Service error stack:', error?.stack)
      console.error('AI Service error details:', JSON.stringify(error, null, 2))
      return this.getFallbackResponse(message, error)
    }
  }

  private async getFallbackResponse(message: string, _error?: any): Promise<string> {
    // Try to use cached data as fallback
    try {
      if (!chatCacheService.isReady()) {
        await chatCacheService.preWarm()
      }

      const results = chatCacheService.fuzzySearch(message)
      if (results.length > 0) {
        return this.formatFallbackResults(results)
      }
    } catch {
      // Ignore cache errors
    }

    // Return helpful guidance instead of error
    const stats = chatCacheService.isReady() ? chatCacheService.getSystemStats() : null
    const statsInfo = stats
      ? `\n\nH·ªá th·ªëng hi·ªán c√≥ ${stats.vehicles} xe, ${stats.drivers} t√†i x·∫ø, ${stats.operators} ƒë∆°n v·ªã v·∫≠n t·∫£i.`
      : ''

    return `Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin b·∫°n c·∫ßn.

**B·∫°n c√≥ th·ªÉ th·ª≠:**
‚Ä¢ T√¨m xe: "xe 98H07480" ho·∫∑c "bi·ªÉn s·ªë 51B12345"
‚Ä¢ T√¨m t√†i x·∫ø: "t√†i x·∫ø Nguy·ªÖn VƒÉn A"
‚Ä¢ T√¨m ƒë∆°n v·ªã: "ƒë∆°n v·ªã Ph∆∞∆°ng Trang"
‚Ä¢ T√¨m tuy·∫øn: "tuy·∫øn TP.HCM - ƒê√† L·∫°t"
‚Ä¢ Th·ªëng k√™: "th·ªëng k√™ ƒëi·ªÅu ƒë·ªô h√¥m nay"
‚Ä¢ T·ªïng quan: "h·ªá th·ªëng c√≥ bao nhi√™u xe"${statsInfo}`
  }

  private formatFallbackResults(results: any[]): string {
    let response = '**K·∫øt qu·∫£ t√¨m ki·∫øm:**\n\n'

    const vehicles = results.filter(r => r._source === 'vehicles')
    const badges = results.filter(r => r._source === 'badges')
    const operators = results.filter(r => r._source === 'operators')
    const drivers = results.filter(r => r._source === 'drivers')
    const routes = results.filter(r => r._source === 'routes')

    if (vehicles.length > 0) {
      response += `**Xe (${vehicles.length}):**\n`
      vehicles.slice(0, 3).forEach(v => {
        const plate = v.plate_number || v.BienSo || 'N/A'
        const type = v.LoaiXe || v.vehicle_type || ''
        response += `‚Ä¢ ${plate}${type ? ` - ${type}` : ''}\n`
      })
      response += '\n'
    }

    if (badges.length > 0) {
      response += `**Ph√π hi·ªáu (${badges.length}):**\n`
      badges.slice(0, 3).forEach(b => {
        const plate = b.BienSoXe || b.plate_number || 'N/A'
        const badgeNum = b.SoPhuHieu || b.badge_number || ''
        response += `‚Ä¢ ${plate}${badgeNum ? ` - PH: ${badgeNum}` : ''}\n`
      })
      response += '\n'
    }

    if (operators.length > 0) {
      response += `**ƒê∆°n v·ªã (${operators.length}):**\n`
      operators.slice(0, 3).forEach(o => {
        const name = o.TenDonVi || o.name || 'N/A'
        response += `‚Ä¢ ${name}\n`
      })
      response += '\n'
    }

    if (drivers.length > 0) {
      response += `**T√†i x·∫ø (${drivers.length}):**\n`
      drivers.slice(0, 3).forEach(d => {
        const name = d.full_name || d.fullName || 'N/A'
        response += `‚Ä¢ ${name}\n`
      })
      response += '\n'
    }

    if (routes.length > 0) {
      response += `**Tuy·∫øn (${routes.length}):**\n`
      routes.slice(0, 3).forEach(r => {
        const code = r.MaSoTuyen || r.route_code || ''
        const departure = r.BenDi || r.departure_station || ''
        const arrival = r.BenDen || r.arrival_station || ''
        response += `‚Ä¢ ${code ? `[${code}] ` : ''}${departure} - ${arrival}\n`
      })
    }

    return response.trim()
  }

  clearHistory(sessionId: string): void {
    this.conversationHistory.delete(sessionId)
  }

  hasApiKey(): boolean {
    return !!GEMINI_API_KEY
  }
}

export const aiService = new AIService()
